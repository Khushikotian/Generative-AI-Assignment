{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4T9k22bK5l8b"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, LeakyReLU, Add\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.datasets import cifar10 # You can replace this with your satellite image dataset\n",
        "\n",
        "# Generator\n",
        "def build_generator(input_shape):\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # Encoder\n",
        "    x = Conv2D(64, (3, 3), strides=1, padding='same')(inputs)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    # Residual blocks\n",
        "    for _ in range(16):\n",
        "        x = residual_block(x, 64)\n",
        "\n",
        "    # Decoder\n",
        "    x = Conv2D(64, (3, 3), strides=1, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Add()([inputs, x])\n",
        "    x = Conv2D(3, (3, 3), strides=1, padding='same', activation='tanh')(x) # Adjust output channels according to image color channels\n",
        "\n",
        "    return Model(inputs, x)\n",
        "\n",
        "def residual_block(x, filters):\n",
        "    y = Conv2D(filters, (3, 3), strides=1, padding='same')(x)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = LeakyReLU(alpha=0.2)(y)\n",
        "    y = Conv2D(filters, (3, 3), strides=1, padding='same')(y)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = Add()([x, y])\n",
        "    return y\n",
        "\n",
        "# Discriminator\n",
        "def build_discriminator(input_shape):\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    x = Conv2D(64, (3, 3), strides=2, padding='same')(inputs)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = Conv2D(128, (3, 3), strides=2, padding='same')(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = Conv2D(256, (3, 3), strides=2, padding='same')(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = Conv2D(512, (3, 3), strides=2, padding='same')(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "    x = tf.keras.layers.Dense(1)(x)\n",
        "\n",
        "    return Model(inputs, x)\n",
        "\n",
        "# Loss function\n",
        "def wasserstein_loss(y_true, y_pred):\n",
        "    return tf.reduce_mean(y_true * y_pred)\n",
        "\n",
        "# Generator input shape (e.g., (256, 256, 3))\n",
        "input_shape = (256, 256, 3)\n",
        "\n",
        "# Build and compile the discriminator\n",
        "discriminator = build_discriminator(input_shape)\n",
        "discriminator.compile(loss=wasserstein_loss, optimizer=Adam(lr=0.0002, beta_1=0.5))\n",
        "\n",
        "# Build the generator\n",
        "generator = build_generator(input_shape)\n",
        "\n",
        "# Build and compile the combined model (GAN)\n",
        "discriminator.trainable = False\n",
        "input_image = Input(shape=input_shape)\n",
        "generated_image = generator(input_image)\n",
        "validity = discriminator(generated_image)\n",
        "combined = Model(input_image, validity)\n",
        "combined.compile(loss=wasserstein_loss, optimizer=Adam(lr=0.0002, beta_1=0.5))\n",
        "\n",
        "# Train the GAN\n",
        "# Load your satellite image dataset or replace this with your data loading mechanism\n",
        "(x_train, _), (_, _) = cifar10.load_data()\n",
        "x_train = x_train / 127.5 - 1.0\n",
        "\n",
        "epochs = 10000\n",
        "batch_size = 32\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
        "    real_images = x_train[idx]\n",
        "\n",
        "    # Train discriminator\n",
        "    noise = np.random.normal(0, 1, (batch_size, *input_shape))\n",
        "    fake_images = generator.predict(noise)\n",
        "    d_loss_real = discriminator.train_on_batch(real_images, -np.ones((batch_size, 1)))\n",
        "    d_loss_fake = discriminator.train_on_batch(fake_images, np.ones((batch_size, 1)))\n",
        "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "    # Train generator\n",
        "    noise = np.random.normal(0, 1, (batch_size, *input_shape))\n",
        "    g_loss = combined.train_on_batch(noise, -np.ones((batch_size, 1)))\n",
        "\n",
        "    # Print progress\n",
        "    print(f\"Epoch {epoch}/{epochs}, D Loss: {d_loss}, G Loss: {g_loss}\")\n",
        "\n",
        "    # Save generated images\n",
        "    if epoch % 100 == 0:\n",
        "        save_generated_images(epoch)\n",
        "\n",
        "# Function to save generated images\n",
        "def save_generated_images(epoch):\n",
        "    noise = np.random.normal(0, 1, (5, *input_shape))\n",
        "    generated_images = generator.predict(noise) * 0.5 + 0.5\n",
        "    for i, image in enumerate(generated_images):\n",
        "        plt.imsave(f\"generated_images/{epoch}_{i}.png\", image)\n"
      ]
    }
  ]
}