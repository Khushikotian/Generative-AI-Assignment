{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.utils import save_image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n"
      ],
      "metadata": {
        "id": "xrtgOXvpSFjj"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the generator network\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(100, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 784),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        img = self.model(z)\n",
        "        return img.view(img.size(0), 1, 28, 28)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Qg5O_IQKSLrN"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the discriminator network\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(784, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        img_flat = img.view(img.size(0), -1)\n",
        "        validity = self.model(img_flat)\n",
        "        return validity\n",
        "\n"
      ],
      "metadata": {
        "id": "WDfaCWPTSSGr"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize networks\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "# Loss function and optimizer\n",
        "adversarial_loss = nn.BCELoss()\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "# Training parameters\n",
        "num_epochs = 200\n",
        "batch_size = 64\n",
        "latent_dim = 100\n",
        "sample_interval = 200\n",
        "\n"
      ],
      "metadata": {
        "id": "tG_QeSL4TEJi"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for i in range(len(data_loader)):\n",
        "        # Adversarial ground truths\n",
        "        valid = torch.ones(batch_size, 1)\n",
        "        fake = torch.zeros(batch_size, 1)\n",
        "\n",
        "        # Generate a batch of images\n",
        "        z = torch.randn(batch_size, latent_dim)\n",
        "        gen_imgs = generator(z)\n",
        "\n",
        "        # Train discriminator\n",
        "        optimizer_D.zero_grad()\n",
        "        real_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
        "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
        "        d_loss = (real_loss + fake_loss) / 2\n",
        "        d_loss.backward(retain_graph=True)  # Set retain_graph=True\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # Train generator\n",
        "        optimizer_G.zero_grad()\n",
        "        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
        "        g_loss.backward(retain_graph=True)  # Set retain_graph=True\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # Print progress\n",
        "        if i % 100 == 0:\n",
        "            print(\n",
        "                f\"[Epoch {epoch}/{num_epochs}] [Batch {i}/{len(data_loader)}] [D loss: {d_loss.item():.6f}] [G loss: {g_loss.item():.6f}]\"\n",
        "            )\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Gv8dgVsTyO-",
        "outputId": "17cf6832-ea87-4f7c-958b-c183febf0823"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 0/200] [Batch 0/16] [D loss: 0.693163] [G loss: 0.697557]\n",
            "[Epoch 1/200] [Batch 0/16] [D loss: 0.693590] [G loss: 0.669914]\n",
            "[Epoch 2/200] [Batch 0/16] [D loss: 0.692946] [G loss: 0.708356]\n",
            "[Epoch 3/200] [Batch 0/16] [D loss: 0.693256] [G loss: 0.706473]\n",
            "[Epoch 4/200] [Batch 0/16] [D loss: 0.695358] [G loss: 0.692029]\n",
            "[Epoch 5/200] [Batch 0/16] [D loss: 0.692916] [G loss: 0.703328]\n",
            "[Epoch 6/200] [Batch 0/16] [D loss: 0.695857] [G loss: 0.710978]\n",
            "[Epoch 7/200] [Batch 0/16] [D loss: 0.689528] [G loss: 0.700532]\n",
            "[Epoch 8/200] [Batch 0/16] [D loss: 0.690515] [G loss: 0.707524]\n",
            "[Epoch 9/200] [Batch 0/16] [D loss: 0.695085] [G loss: 0.689372]\n",
            "[Epoch 10/200] [Batch 0/16] [D loss: 0.689901] [G loss: 0.714130]\n",
            "[Epoch 11/200] [Batch 0/16] [D loss: 0.693741] [G loss: 0.685341]\n",
            "[Epoch 12/200] [Batch 0/16] [D loss: 0.696112] [G loss: 0.722313]\n",
            "[Epoch 13/200] [Batch 0/16] [D loss: 0.695022] [G loss: 0.702495]\n",
            "[Epoch 14/200] [Batch 0/16] [D loss: 0.695686] [G loss: 0.699445]\n",
            "[Epoch 15/200] [Batch 0/16] [D loss: 0.697393] [G loss: 0.722458]\n",
            "[Epoch 16/200] [Batch 0/16] [D loss: 0.694224] [G loss: 0.709484]\n",
            "[Epoch 17/200] [Batch 0/16] [D loss: 0.693924] [G loss: 0.695465]\n",
            "[Epoch 18/200] [Batch 0/16] [D loss: 0.695262] [G loss: 0.688358]\n",
            "[Epoch 19/200] [Batch 0/16] [D loss: 0.698419] [G loss: 0.711623]\n",
            "[Epoch 20/200] [Batch 0/16] [D loss: 0.692717] [G loss: 0.702744]\n",
            "[Epoch 21/200] [Batch 0/16] [D loss: 0.687654] [G loss: 0.679608]\n",
            "[Epoch 22/200] [Batch 0/16] [D loss: 0.692470] [G loss: 0.670220]\n",
            "[Epoch 23/200] [Batch 0/16] [D loss: 0.692692] [G loss: 0.718102]\n",
            "[Epoch 24/200] [Batch 0/16] [D loss: 0.695805] [G loss: 0.700854]\n",
            "[Epoch 25/200] [Batch 0/16] [D loss: 0.689981] [G loss: 0.706772]\n",
            "[Epoch 26/200] [Batch 0/16] [D loss: 0.691836] [G loss: 0.700997]\n",
            "[Epoch 27/200] [Batch 0/16] [D loss: 0.694203] [G loss: 0.688858]\n",
            "[Epoch 28/200] [Batch 0/16] [D loss: 0.696195] [G loss: 0.701669]\n",
            "[Epoch 29/200] [Batch 0/16] [D loss: 0.692564] [G loss: 0.693776]\n",
            "[Epoch 30/200] [Batch 0/16] [D loss: 0.695179] [G loss: 0.716554]\n",
            "[Epoch 31/200] [Batch 0/16] [D loss: 0.695748] [G loss: 0.703528]\n",
            "[Epoch 32/200] [Batch 0/16] [D loss: 0.693616] [G loss: 0.686874]\n",
            "[Epoch 33/200] [Batch 0/16] [D loss: 0.695010] [G loss: 0.703370]\n",
            "[Epoch 34/200] [Batch 0/16] [D loss: 0.690590] [G loss: 0.722736]\n",
            "[Epoch 35/200] [Batch 0/16] [D loss: 0.694678] [G loss: 0.696756]\n",
            "[Epoch 36/200] [Batch 0/16] [D loss: 0.697139] [G loss: 0.700049]\n",
            "[Epoch 37/200] [Batch 0/16] [D loss: 0.695662] [G loss: 0.699982]\n",
            "[Epoch 38/200] [Batch 0/16] [D loss: 0.690945] [G loss: 0.702338]\n",
            "[Epoch 39/200] [Batch 0/16] [D loss: 0.694064] [G loss: 0.708121]\n",
            "[Epoch 40/200] [Batch 0/16] [D loss: 0.690747] [G loss: 0.688905]\n",
            "[Epoch 41/200] [Batch 0/16] [D loss: 0.694088] [G loss: 0.708105]\n",
            "[Epoch 42/200] [Batch 0/16] [D loss: 0.694662] [G loss: 0.719603]\n",
            "[Epoch 43/200] [Batch 0/16] [D loss: 0.695318] [G loss: 0.695277]\n",
            "[Epoch 44/200] [Batch 0/16] [D loss: 0.693184] [G loss: 0.688329]\n",
            "[Epoch 45/200] [Batch 0/16] [D loss: 0.695724] [G loss: 0.715567]\n",
            "[Epoch 46/200] [Batch 0/16] [D loss: 0.692114] [G loss: 0.697999]\n",
            "[Epoch 47/200] [Batch 0/16] [D loss: 0.693979] [G loss: 0.714939]\n",
            "[Epoch 48/200] [Batch 0/16] [D loss: 0.696213] [G loss: 0.680434]\n",
            "[Epoch 49/200] [Batch 0/16] [D loss: 0.687068] [G loss: 0.699358]\n",
            "[Epoch 50/200] [Batch 0/16] [D loss: 0.697490] [G loss: 0.703112]\n",
            "[Epoch 51/200] [Batch 0/16] [D loss: 0.688855] [G loss: 0.705387]\n",
            "[Epoch 52/200] [Batch 0/16] [D loss: 0.692398] [G loss: 0.724008]\n",
            "[Epoch 53/200] [Batch 0/16] [D loss: 0.697935] [G loss: 0.696415]\n",
            "[Epoch 54/200] [Batch 0/16] [D loss: 0.698430] [G loss: 0.696707]\n",
            "[Epoch 55/200] [Batch 0/16] [D loss: 0.691754] [G loss: 0.705003]\n",
            "[Epoch 56/200] [Batch 0/16] [D loss: 0.694394] [G loss: 0.707418]\n",
            "[Epoch 57/200] [Batch 0/16] [D loss: 0.691615] [G loss: 0.687879]\n",
            "[Epoch 58/200] [Batch 0/16] [D loss: 0.691595] [G loss: 0.701877]\n",
            "[Epoch 59/200] [Batch 0/16] [D loss: 0.692504] [G loss: 0.682788]\n",
            "[Epoch 60/200] [Batch 0/16] [D loss: 0.695120] [G loss: 0.711313]\n",
            "[Epoch 61/200] [Batch 0/16] [D loss: 0.694263] [G loss: 0.697048]\n",
            "[Epoch 62/200] [Batch 0/16] [D loss: 0.694000] [G loss: 0.701133]\n",
            "[Epoch 63/200] [Batch 0/16] [D loss: 0.692979] [G loss: 0.698916]\n",
            "[Epoch 64/200] [Batch 0/16] [D loss: 0.691134] [G loss: 0.661946]\n",
            "[Epoch 65/200] [Batch 0/16] [D loss: 0.693469] [G loss: 0.694398]\n",
            "[Epoch 66/200] [Batch 0/16] [D loss: 0.692054] [G loss: 0.711103]\n",
            "[Epoch 67/200] [Batch 0/16] [D loss: 0.694872] [G loss: 0.698951]\n",
            "[Epoch 68/200] [Batch 0/16] [D loss: 0.694225] [G loss: 0.697089]\n",
            "[Epoch 69/200] [Batch 0/16] [D loss: 0.692989] [G loss: 0.673645]\n",
            "[Epoch 70/200] [Batch 0/16] [D loss: 0.697279] [G loss: 0.701686]\n",
            "[Epoch 71/200] [Batch 0/16] [D loss: 0.693006] [G loss: 0.709921]\n",
            "[Epoch 72/200] [Batch 0/16] [D loss: 0.693336] [G loss: 0.696509]\n",
            "[Epoch 73/200] [Batch 0/16] [D loss: 0.697514] [G loss: 0.699175]\n",
            "[Epoch 74/200] [Batch 0/16] [D loss: 0.691906] [G loss: 0.704971]\n",
            "[Epoch 75/200] [Batch 0/16] [D loss: 0.693994] [G loss: 0.707042]\n",
            "[Epoch 76/200] [Batch 0/16] [D loss: 0.693901] [G loss: 0.699635]\n",
            "[Epoch 77/200] [Batch 0/16] [D loss: 0.693483] [G loss: 0.696513]\n",
            "[Epoch 78/200] [Batch 0/16] [D loss: 0.697236] [G loss: 0.681712]\n",
            "[Epoch 79/200] [Batch 0/16] [D loss: 0.695313] [G loss: 0.708817]\n",
            "[Epoch 80/200] [Batch 0/16] [D loss: 0.695843] [G loss: 0.711312]\n",
            "[Epoch 81/200] [Batch 0/16] [D loss: 0.689592] [G loss: 0.688020]\n",
            "[Epoch 82/200] [Batch 0/16] [D loss: 0.690943] [G loss: 0.699292]\n",
            "[Epoch 83/200] [Batch 0/16] [D loss: 0.693725] [G loss: 0.708174]\n",
            "[Epoch 84/200] [Batch 0/16] [D loss: 0.696104] [G loss: 0.695541]\n",
            "[Epoch 85/200] [Batch 0/16] [D loss: 0.694928] [G loss: 0.698111]\n",
            "[Epoch 86/200] [Batch 0/16] [D loss: 0.695211] [G loss: 0.684002]\n",
            "[Epoch 87/200] [Batch 0/16] [D loss: 0.690557] [G loss: 0.696584]\n",
            "[Epoch 88/200] [Batch 0/16] [D loss: 0.689494] [G loss: 0.688825]\n",
            "[Epoch 89/200] [Batch 0/16] [D loss: 0.692318] [G loss: 0.690672]\n",
            "[Epoch 90/200] [Batch 0/16] [D loss: 0.689464] [G loss: 0.704311]\n",
            "[Epoch 91/200] [Batch 0/16] [D loss: 0.691048] [G loss: 0.676947]\n",
            "[Epoch 92/200] [Batch 0/16] [D loss: 0.693702] [G loss: 0.701317]\n",
            "[Epoch 93/200] [Batch 0/16] [D loss: 0.692845] [G loss: 0.705014]\n",
            "[Epoch 94/200] [Batch 0/16] [D loss: 0.691983] [G loss: 0.703693]\n",
            "[Epoch 95/200] [Batch 0/16] [D loss: 0.689857] [G loss: 0.680217]\n",
            "[Epoch 96/200] [Batch 0/16] [D loss: 0.698733] [G loss: 0.704052]\n",
            "[Epoch 97/200] [Batch 0/16] [D loss: 0.691055] [G loss: 0.694624]\n",
            "[Epoch 98/200] [Batch 0/16] [D loss: 0.694885] [G loss: 0.685694]\n",
            "[Epoch 99/200] [Batch 0/16] [D loss: 0.695138] [G loss: 0.705853]\n",
            "[Epoch 100/200] [Batch 0/16] [D loss: 0.692643] [G loss: 0.715039]\n",
            "[Epoch 101/200] [Batch 0/16] [D loss: 0.693212] [G loss: 0.688344]\n",
            "[Epoch 102/200] [Batch 0/16] [D loss: 0.692069] [G loss: 0.677789]\n",
            "[Epoch 103/200] [Batch 0/16] [D loss: 0.695738] [G loss: 0.679931]\n",
            "[Epoch 104/200] [Batch 0/16] [D loss: 0.691413] [G loss: 0.686041]\n",
            "[Epoch 105/200] [Batch 0/16] [D loss: 0.692123] [G loss: 0.696554]\n",
            "[Epoch 106/200] [Batch 0/16] [D loss: 0.696821] [G loss: 0.680542]\n",
            "[Epoch 107/200] [Batch 0/16] [D loss: 0.692851] [G loss: 0.694436]\n",
            "[Epoch 108/200] [Batch 0/16] [D loss: 0.690039] [G loss: 0.692569]\n",
            "[Epoch 109/200] [Batch 0/16] [D loss: 0.694017] [G loss: 0.696270]\n",
            "[Epoch 110/200] [Batch 0/16] [D loss: 0.689967] [G loss: 0.690449]\n",
            "[Epoch 111/200] [Batch 0/16] [D loss: 0.694505] [G loss: 0.718393]\n",
            "[Epoch 112/200] [Batch 0/16] [D loss: 0.694652] [G loss: 0.690228]\n",
            "[Epoch 113/200] [Batch 0/16] [D loss: 0.693841] [G loss: 0.691806]\n",
            "[Epoch 114/200] [Batch 0/16] [D loss: 0.694727] [G loss: 0.683454]\n",
            "[Epoch 115/200] [Batch 0/16] [D loss: 0.696562] [G loss: 0.698412]\n",
            "[Epoch 116/200] [Batch 0/16] [D loss: 0.691155] [G loss: 0.705464]\n",
            "[Epoch 117/200] [Batch 0/16] [D loss: 0.693613] [G loss: 0.699085]\n",
            "[Epoch 118/200] [Batch 0/16] [D loss: 0.692674] [G loss: 0.698341]\n",
            "[Epoch 119/200] [Batch 0/16] [D loss: 0.693206] [G loss: 0.693641]\n",
            "[Epoch 120/200] [Batch 0/16] [D loss: 0.693055] [G loss: 0.699197]\n",
            "[Epoch 121/200] [Batch 0/16] [D loss: 0.693617] [G loss: 0.696788]\n",
            "[Epoch 122/200] [Batch 0/16] [D loss: 0.692533] [G loss: 0.685244]\n",
            "[Epoch 123/200] [Batch 0/16] [D loss: 0.693039] [G loss: 0.684241]\n",
            "[Epoch 124/200] [Batch 0/16] [D loss: 0.692726] [G loss: 0.702090]\n",
            "[Epoch 125/200] [Batch 0/16] [D loss: 0.690217] [G loss: 0.701504]\n",
            "[Epoch 126/200] [Batch 0/16] [D loss: 0.692878] [G loss: 0.699939]\n",
            "[Epoch 127/200] [Batch 0/16] [D loss: 0.693168] [G loss: 0.697678]\n",
            "[Epoch 128/200] [Batch 0/16] [D loss: 0.696464] [G loss: 0.706635]\n",
            "[Epoch 129/200] [Batch 0/16] [D loss: 0.693640] [G loss: 0.698105]\n",
            "[Epoch 130/200] [Batch 0/16] [D loss: 0.691894] [G loss: 0.692131]\n",
            "[Epoch 131/200] [Batch 0/16] [D loss: 0.692402] [G loss: 0.680982]\n",
            "[Epoch 132/200] [Batch 0/16] [D loss: 0.693918] [G loss: 0.697473]\n",
            "[Epoch 133/200] [Batch 0/16] [D loss: 0.694767] [G loss: 0.683082]\n",
            "[Epoch 134/200] [Batch 0/16] [D loss: 0.696411] [G loss: 0.685141]\n",
            "[Epoch 135/200] [Batch 0/16] [D loss: 0.693200] [G loss: 0.698411]\n",
            "[Epoch 136/200] [Batch 0/16] [D loss: 0.688782] [G loss: 0.677863]\n",
            "[Epoch 137/200] [Batch 0/16] [D loss: 0.693151] [G loss: 0.689296]\n",
            "[Epoch 138/200] [Batch 0/16] [D loss: 0.691875] [G loss: 0.697403]\n",
            "[Epoch 139/200] [Batch 0/16] [D loss: 0.693887] [G loss: 0.681069]\n",
            "[Epoch 140/200] [Batch 0/16] [D loss: 0.694936] [G loss: 0.691820]\n",
            "[Epoch 141/200] [Batch 0/16] [D loss: 0.696092] [G loss: 0.699618]\n",
            "[Epoch 142/200] [Batch 0/16] [D loss: 0.696421] [G loss: 0.694564]\n",
            "[Epoch 143/200] [Batch 0/16] [D loss: 0.692241] [G loss: 0.691830]\n",
            "[Epoch 144/200] [Batch 0/16] [D loss: 0.692208] [G loss: 0.689103]\n",
            "[Epoch 145/200] [Batch 0/16] [D loss: 0.694521] [G loss: 0.700184]\n",
            "[Epoch 146/200] [Batch 0/16] [D loss: 0.694917] [G loss: 0.691404]\n",
            "[Epoch 147/200] [Batch 0/16] [D loss: 0.696050] [G loss: 0.706661]\n",
            "[Epoch 148/200] [Batch 0/16] [D loss: 0.694350] [G loss: 0.704883]\n",
            "[Epoch 149/200] [Batch 0/16] [D loss: 0.695440] [G loss: 0.684842]\n",
            "[Epoch 150/200] [Batch 0/16] [D loss: 0.696620] [G loss: 0.694636]\n",
            "[Epoch 151/200] [Batch 0/16] [D loss: 0.697791] [G loss: 0.673773]\n",
            "[Epoch 152/200] [Batch 0/16] [D loss: 0.697324] [G loss: 0.692697]\n",
            "[Epoch 153/200] [Batch 0/16] [D loss: 0.695029] [G loss: 0.699784]\n",
            "[Epoch 154/200] [Batch 0/16] [D loss: 0.694313] [G loss: 0.688423]\n",
            "[Epoch 155/200] [Batch 0/16] [D loss: 0.693663] [G loss: 0.695573]\n",
            "[Epoch 156/200] [Batch 0/16] [D loss: 0.696044] [G loss: 0.679047]\n",
            "[Epoch 157/200] [Batch 0/16] [D loss: 0.694284] [G loss: 0.706026]\n",
            "[Epoch 158/200] [Batch 0/16] [D loss: 0.695627] [G loss: 0.677127]\n",
            "[Epoch 159/200] [Batch 0/16] [D loss: 0.691974] [G loss: 0.699788]\n",
            "[Epoch 160/200] [Batch 0/16] [D loss: 0.694974] [G loss: 0.698037]\n",
            "[Epoch 161/200] [Batch 0/16] [D loss: 0.695774] [G loss: 0.691853]\n",
            "[Epoch 162/200] [Batch 0/16] [D loss: 0.691263] [G loss: 0.688203]\n",
            "[Epoch 163/200] [Batch 0/16] [D loss: 0.693439] [G loss: 0.687842]\n",
            "[Epoch 164/200] [Batch 0/16] [D loss: 0.692054] [G loss: 0.689971]\n",
            "[Epoch 165/200] [Batch 0/16] [D loss: 0.695048] [G loss: 0.696155]\n",
            "[Epoch 166/200] [Batch 0/16] [D loss: 0.693920] [G loss: 0.700915]\n",
            "[Epoch 167/200] [Batch 0/16] [D loss: 0.693243] [G loss: 0.696162]\n",
            "[Epoch 168/200] [Batch 0/16] [D loss: 0.692720] [G loss: 0.690385]\n",
            "[Epoch 169/200] [Batch 0/16] [D loss: 0.695039] [G loss: 0.705027]\n",
            "[Epoch 170/200] [Batch 0/16] [D loss: 0.694334] [G loss: 0.702343]\n",
            "[Epoch 171/200] [Batch 0/16] [D loss: 0.694934] [G loss: 0.694631]\n",
            "[Epoch 172/200] [Batch 0/16] [D loss: 0.695286] [G loss: 0.702670]\n",
            "[Epoch 173/200] [Batch 0/16] [D loss: 0.693983] [G loss: 0.712301]\n",
            "[Epoch 174/200] [Batch 0/16] [D loss: 0.691652] [G loss: 0.716123]\n",
            "[Epoch 175/200] [Batch 0/16] [D loss: 0.692563] [G loss: 0.689784]\n",
            "[Epoch 176/200] [Batch 0/16] [D loss: 0.693693] [G loss: 0.685114]\n",
            "[Epoch 177/200] [Batch 0/16] [D loss: 0.695432] [G loss: 0.715383]\n",
            "[Epoch 178/200] [Batch 0/16] [D loss: 0.694875] [G loss: 0.687900]\n",
            "[Epoch 179/200] [Batch 0/16] [D loss: 0.690998] [G loss: 0.689084]\n",
            "[Epoch 180/200] [Batch 0/16] [D loss: 0.692324] [G loss: 0.688188]\n",
            "[Epoch 181/200] [Batch 0/16] [D loss: 0.692756] [G loss: 0.704212]\n",
            "[Epoch 182/200] [Batch 0/16] [D loss: 0.693830] [G loss: 0.695338]\n",
            "[Epoch 183/200] [Batch 0/16] [D loss: 0.694608] [G loss: 0.697928]\n",
            "[Epoch 184/200] [Batch 0/16] [D loss: 0.691989] [G loss: 0.693276]\n",
            "[Epoch 185/200] [Batch 0/16] [D loss: 0.693057] [G loss: 0.705205]\n",
            "[Epoch 186/200] [Batch 0/16] [D loss: 0.692923] [G loss: 0.702513]\n",
            "[Epoch 187/200] [Batch 0/16] [D loss: 0.693395] [G loss: 0.691738]\n",
            "[Epoch 188/200] [Batch 0/16] [D loss: 0.695037] [G loss: 0.690594]\n",
            "[Epoch 189/200] [Batch 0/16] [D loss: 0.694866] [G loss: 0.688087]\n",
            "[Epoch 190/200] [Batch 0/16] [D loss: 0.696711] [G loss: 0.685579]\n",
            "[Epoch 191/200] [Batch 0/16] [D loss: 0.693858] [G loss: 0.692114]\n",
            "[Epoch 192/200] [Batch 0/16] [D loss: 0.692721] [G loss: 0.690418]\n",
            "[Epoch 193/200] [Batch 0/16] [D loss: 0.696581] [G loss: 0.697636]\n",
            "[Epoch 194/200] [Batch 0/16] [D loss: 0.694435] [G loss: 0.696077]\n",
            "[Epoch 195/200] [Batch 0/16] [D loss: 0.690546] [G loss: 0.708774]\n",
            "[Epoch 196/200] [Batch 0/16] [D loss: 0.694843] [G loss: 0.685240]\n",
            "[Epoch 197/200] [Batch 0/16] [D loss: 0.689682] [G loss: 0.695405]\n",
            "[Epoch 198/200] [Batch 0/16] [D loss: 0.694597] [G loss: 0.697582]\n",
            "[Epoch 199/200] [Batch 0/16] [D loss: 0.692565] [G loss: 0.697755]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Save generated images at sample interval\n",
        "if epoch % sample_interval == 0 and i == 0:\n",
        "            save_image(gen_imgs.data[:25], f\"images/{epoch}.png\", nrow=5, normalize=True)"
      ],
      "metadata": {
        "id": "i8OD6ljMVHzb"
      },
      "execution_count": 75,
      "outputs": []
    }
  ]
}