# -*- coding: utf-8 -*-
"""AstrophotographyEnhancement.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15VlVoKC2RasjZwpYKktsYCSg-sVZuFRs
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
from torchvision.transforms import ToTensor
from torch.utils.data import DataLoader

# Define the device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

import torch
from torch.utils.data import Dataset
import numpy as np

class CustomDataset(Dataset):
    def __init__(self, num_samples=1000, img_size=(64, 64)):
        self.num_samples = num_samples
        self.img_size = img_size

    def __len__(self):
        return self.num_samples

    def __getitem__(self, idx):
        # Generate random noise as input to the generator
        noise = np.random.randn(*self.img_size)
        # Generate random image with values between 0 and 1
        image = np.random.rand(*self.img_size)
        # Convert image and noise to PyTorch tensors
        image = torch.tensor(image, dtype=torch.float32)
        noise = torch.tensor(noise, dtype=torch.float32)
        # Add channel dimension for grayscale images
        image = image.unsqueeze(0)
        noise = noise.unsqueeze(0)
        return noise, image

# Create an instance of the dataset
dataset = CustomDataset(num_samples=1000, img_size=(64, 64))

#  Define the discriminator
class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.model = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(256, 1, kernel_size=4, stride=1, padding=0),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.model(x)

# Define the generator
class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()
        self.model = nn.Sequential(
            nn.ConvTranspose2d(100, 256, kernel_size=4, stride=1, padding=0),
            nn.BatchNorm2d(256),
            nn.ReLU(True),
            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(True),
            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(True),
            nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1),
            nn.Tanh()
        )

    def forward(self, x):
        return self.model(x)

# Define transforms for the dataset
transform = ToTensor()

# Create a DataLoader
batch_size = 64
data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Define the discriminator and generator
discriminator = Discriminator().to(device)
generator = Generator().to(device)

# Define the optimizers
optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))
optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))

# Define the loss function
criterion = nn.BCELoss()

# Training loop
num_epochs = 10
for epoch in range(num_epochs):
    for i, data in enumerate(data_loader):
        # Train Discriminator
        optimizer_D.zero_grad()
        real_images = data.to(device)
        real_labels = torch.ones(real_images.size(0), 1, 5, 5).to(device)  # Match discriminator output shape
        fake_labels = torch.zeros(real_images.size(0), 1, 5, 5).to(device)  # Match discriminator output shape

        # Train with real images
        real_outputs = discriminator(real_images)
        d_loss_real = criterion(real_outputs, real_labels)
        d_loss_real.backward()

        # Train with fake images
        noise = torch.randn(real_images.size(0), 3, 64, 64).to(device)  # Assuming RGB images, adjust the number of channels accordingly
        fake_images = generator(noise)
        fake_outputs = discriminator(fake_images.detach())
        d_loss_fake = criterion(fake_outputs, fake_labels)
        d_loss_fake.backward()

        d_loss = d_loss_real + d_loss_fake
        optimizer_D.step()

        # Train Generator
        optimizer_G.zero_grad()
        noise = torch.randn(real_images.size(0), 100, 1, 1).to(device)
        fake_images = generator(noise)
        outputs = discriminator(fake_images)
        g_loss = criterion(outputs, real_labels)  # Using real_labels as discriminator tries to classify fake images as real
        g_loss.backward()
        optimizer_G.step()

        if (i+1) % 100 == 0:
            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(data_loader)}], '
                  f'Discriminator Loss: {d_loss.item():.4f}, Generator Loss: {g_loss.item():.4f}')