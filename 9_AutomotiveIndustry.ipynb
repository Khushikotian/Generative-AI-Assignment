{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOBQfbP3JnLQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Define the Generator model\n",
        "def build_generator(input_shape):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(128 * 7 * 7, activation=\"relu\", input_dim=latent_dim))\n",
        "    model.add(layers.Reshape((7, 7, 128)))\n",
        "    model.add(layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\", activation=\"relu\"))\n",
        "    model.add(layers.Conv2DTranspose(64, kernel_size=4, strides=2, padding=\"same\", activation=\"relu\"))\n",
        "    model.add(layers.Conv2D(3, kernel_size=7, activation=\"sigmoid\", padding=\"same\"))\n",
        "    return model\n",
        "\n",
        "# Define the Discriminator model\n",
        "def build_discriminator(input_shape):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Conv2D(64, kernel_size=3, strides=2, input_shape=input_shape, padding=\"same\"))\n",
        "    model.add(layers.LeakyReLU(alpha=0.2))\n",
        "    model.add(layers.Dropout(0.25))\n",
        "    model.add(layers.Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
        "    model.add(layers.LeakyReLU(alpha=0.2))\n",
        "    model.add(layers.Dropout(0.25))\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "# Define the GAN model\n",
        "def build_gan(generator, discriminator):\n",
        "    discriminator.trainable = False\n",
        "    gan_input = layers.Input(shape=(latent_dim,))\n",
        "    gan_output = discriminator(generator(gan_input))\n",
        "    gan = models.Model(gan_input, gan_output)\n",
        "    return gan\n",
        "\n",
        "# Load and preprocess automotive images\n",
        "def load_automotive_images(file_paths):\n",
        "    # Load images and preprocess (e.g., normalization)\n",
        "    return preprocessed_images\n",
        "\n",
        "# Add noise to automotive images\n",
        "def add_noise(images, noise_factor):\n",
        "    noisy_images = images + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=images.shape)\n",
        "    noisy_images = np.clip(noisy_images, 0., 1.)  # Clip values to [0, 1]\n",
        "    return noisy_images\n",
        "\n",
        "# Define training parameters\n",
        "epochs = 100\n",
        "batch_size = 32\n",
        "noise_factor = 0.5\n",
        "latent_dim = 100\n",
        "\n",
        "# Build and compile the discriminator\n",
        "input_shape = (128, 128, 3)  # Adjust this according to the size of your input images\n",
        "discriminator = build_discriminator(input_shape)\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "\n",
        "# Build the generator\n",
        "generator = build_generator(input_shape)\n",
        "\n",
        "# Build and compile the GAN model\n",
        "gan = build_gan(generator, discriminator)\n",
        "gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "\n",
        "# Load and preprocess automotive images\n",
        "file_paths = [...]  # List of file paths to automotive images\n",
        "images = load_automotive_images(file_paths)\n",
        "\n",
        "# Add noise to the images\n",
        "noisy_images = add_noise(images, noise_factor)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    # Select a random batch of images\n",
        "    idx = np.random.randint(0, images.shape[0], batch_size)\n",
        "    real_images = images[idx]\n",
        "\n",
        "    # Generate a batch of noisy images\n",
        "    idx = np.random.randint(0, noisy_images.shape[0], batch_size)\n",
        "    noisy_batch = noisy_images[idx]\n",
        "\n",
        "    # Generate fake images from noisy images\n",
        "    generated_images = generator.predict(noisy_batch)\n",
        "\n",
        "    # Train the discriminator\n",
        "    d_loss_real = discriminator.train_on_batch(real_images, np.ones(batch_size))\n",
        "    d_loss_fake = discriminator.train_on_batch(generated_images, np.zeros(batch_size))\n",
        "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "    # Train the generator (via the GAN model)\n",
        "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "    valid_labels = np.array([1] * batch_size)\n",
        "    g_loss = gan.train_on_batch(noise, valid_labels)\n",
        "\n",
        "    # Print progress\n",
        "    print(f\"Epoch {epoch}, D Loss: {d_loss}, G Loss: {g_loss}\")\n",
        "\n",
        "# After training, you can use the generator to denoise new images\n",
        "denoised_images = generator.predict(new_noisy_images)\n"
      ]
    }
  ]
}